{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 390,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 10.71449089050293,
      "learning_rate": 0.00019846153846153847,
      "loss": 22.655870056152345,
      "step": 10
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 10.232928276062012,
      "learning_rate": 0.00019461538461538463,
      "loss": 5.933728790283203,
      "step": 20
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 15.738134384155273,
      "learning_rate": 0.0001907692307692308,
      "loss": 5.803728103637695,
      "step": 30
    },
    {
      "epoch": 0.30828516377649323,
      "grad_norm": 13.918347358703613,
      "learning_rate": 0.00018692307692307693,
      "loss": 5.635154342651367,
      "step": 40
    },
    {
      "epoch": 0.3853564547206166,
      "grad_norm": 15.645668983459473,
      "learning_rate": 0.0001830769230769231,
      "loss": 5.672851181030273,
      "step": 50
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 9.409565925598145,
      "learning_rate": 0.00017923076923076925,
      "loss": 5.497320175170898,
      "step": 60
    },
    {
      "epoch": 0.5394990366088632,
      "grad_norm": 8.557103157043457,
      "learning_rate": 0.0001753846153846154,
      "loss": 5.850469589233398,
      "step": 70
    },
    {
      "epoch": 0.6165703275529865,
      "grad_norm": 7.390137195587158,
      "learning_rate": 0.00017153846153846153,
      "loss": 5.624962615966797,
      "step": 80
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 6.91805362701416,
      "learning_rate": 0.00016769230769230772,
      "loss": 5.702191162109375,
      "step": 90
    },
    {
      "epoch": 0.7707129094412332,
      "grad_norm": 8.740006446838379,
      "learning_rate": 0.00016384615384615385,
      "loss": 5.662079238891602,
      "step": 100
    },
    {
      "epoch": 0.8477842003853564,
      "grad_norm": 9.592401504516602,
      "learning_rate": 0.00016,
      "loss": 5.896202087402344,
      "step": 110
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 12.15601634979248,
      "learning_rate": 0.00015615384615384615,
      "loss": 5.440667343139649,
      "step": 120
    },
    {
      "epoch": 1.0,
      "grad_norm": 10.161372184753418,
      "learning_rate": 0.0001523076923076923,
      "loss": 5.485287094116211,
      "step": 130
    },
    {
      "epoch": 1.0770712909441233,
      "grad_norm": 9.676664352416992,
      "learning_rate": 0.00014846153846153847,
      "loss": 5.823320007324218,
      "step": 140
    },
    {
      "epoch": 1.1541425818882467,
      "grad_norm": 10.996055603027344,
      "learning_rate": 0.0001446153846153846,
      "loss": 5.596116256713867,
      "step": 150
    },
    {
      "epoch": 1.2312138728323698,
      "grad_norm": 4.644158363342285,
      "learning_rate": 0.0001407692307692308,
      "loss": 5.58727912902832,
      "step": 160
    },
    {
      "epoch": 1.3082851637764932,
      "grad_norm": 7.662019729614258,
      "learning_rate": 0.00013692307692307693,
      "loss": 5.501605606079101,
      "step": 170
    },
    {
      "epoch": 1.3853564547206165,
      "grad_norm": 10.551322937011719,
      "learning_rate": 0.00013307692307692307,
      "loss": 5.762403106689453,
      "step": 180
    },
    {
      "epoch": 1.4624277456647399,
      "grad_norm": 5.634364128112793,
      "learning_rate": 0.00012923076923076923,
      "loss": 5.585348510742188,
      "step": 190
    },
    {
      "epoch": 1.5394990366088632,
      "grad_norm": 8.315348625183105,
      "learning_rate": 0.0001253846153846154,
      "loss": 5.573325729370117,
      "step": 200
    },
    {
      "epoch": 1.6165703275529864,
      "grad_norm": 9.58388900756836,
      "learning_rate": 0.00012153846153846153,
      "loss": 5.675560379028321,
      "step": 210
    },
    {
      "epoch": 1.69364161849711,
      "grad_norm": 6.323094844818115,
      "learning_rate": 0.0001176923076923077,
      "loss": 5.519981384277344,
      "step": 220
    },
    {
      "epoch": 1.770712909441233,
      "grad_norm": 10.932339668273926,
      "learning_rate": 0.00011384615384615384,
      "loss": 5.6203857421875,
      "step": 230
    },
    {
      "epoch": 1.8477842003853564,
      "grad_norm": 4.912787437438965,
      "learning_rate": 0.00011000000000000002,
      "loss": 5.724158477783203,
      "step": 240
    },
    {
      "epoch": 1.9248554913294798,
      "grad_norm": 7.204580307006836,
      "learning_rate": 0.00010615384615384615,
      "loss": 5.380632400512695,
      "step": 250
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.543238639831543,
      "learning_rate": 0.0001023076923076923,
      "loss": 5.320892715454102,
      "step": 260
    },
    {
      "epoch": 2.077071290944123,
      "grad_norm": 11.830482482910156,
      "learning_rate": 9.846153846153848e-05,
      "loss": 5.4537403106689455,
      "step": 270
    },
    {
      "epoch": 2.1541425818882467,
      "grad_norm": 10.95028305053711,
      "learning_rate": 9.461538461538461e-05,
      "loss": 5.461973190307617,
      "step": 280
    },
    {
      "epoch": 2.23121387283237,
      "grad_norm": 11.872149467468262,
      "learning_rate": 9.076923076923078e-05,
      "loss": 5.336160278320312,
      "step": 290
    },
    {
      "epoch": 2.3082851637764934,
      "grad_norm": 36.60530090332031,
      "learning_rate": 8.692307692307692e-05,
      "loss": 5.332376098632812,
      "step": 300
    },
    {
      "epoch": 2.3853564547206165,
      "grad_norm": 15.51420783996582,
      "learning_rate": 8.307692307692309e-05,
      "loss": 5.194593811035157,
      "step": 310
    },
    {
      "epoch": 2.4624277456647397,
      "grad_norm": 13.756877899169922,
      "learning_rate": 7.923076923076924e-05,
      "loss": 4.796422958374023,
      "step": 320
    },
    {
      "epoch": 2.5394990366088632,
      "grad_norm": 12.61147403717041,
      "learning_rate": 7.538461538461539e-05,
      "loss": 4.466828918457031,
      "step": 330
    },
    {
      "epoch": 2.6165703275529864,
      "grad_norm": 8.995928764343262,
      "learning_rate": 7.153846153846155e-05,
      "loss": 4.950369262695313,
      "step": 340
    },
    {
      "epoch": 2.69364161849711,
      "grad_norm": 13.254773139953613,
      "learning_rate": 6.76923076923077e-05,
      "loss": 4.582786178588867,
      "step": 350
    },
    {
      "epoch": 2.770712909441233,
      "grad_norm": 8.542901039123535,
      "learning_rate": 6.384615384615385e-05,
      "loss": 3.856131744384766,
      "step": 360
    },
    {
      "epoch": 2.847784200385356,
      "grad_norm": 14.116633415222168,
      "learning_rate": 6e-05,
      "loss": 5.095643997192383,
      "step": 370
    },
    {
      "epoch": 2.9248554913294798,
      "grad_norm": 8.257776260375977,
      "learning_rate": 5.615384615384616e-05,
      "loss": 4.092145538330078,
      "step": 380
    },
    {
      "epoch": 3.0,
      "grad_norm": 6.7490105628967285,
      "learning_rate": 5.230769230769231e-05,
      "loss": 4.387210464477539,
      "step": 390
    }
  ],
  "logging_steps": 10,
  "max_steps": 520,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.904736967929037e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
