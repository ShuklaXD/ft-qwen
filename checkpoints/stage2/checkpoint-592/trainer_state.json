{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 592,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 36.1967658996582,
      "learning_rate": 0.00019230769230769233,
      "loss": 37.584930419921875,
      "step": 10
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 38.25251007080078,
      "learning_rate": 0.00017692307692307693,
      "loss": 7.575152587890625,
      "step": 20
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 14.819173812866211,
      "learning_rate": 0.00016153846153846155,
      "loss": 7.883622741699218,
      "step": 30
    },
    {
      "epoch": 0.30828516377649323,
      "grad_norm": 33.081573486328125,
      "learning_rate": 0.00014615384615384615,
      "loss": 6.81710205078125,
      "step": 40
    },
    {
      "epoch": 0.3853564547206166,
      "grad_norm": 19.220104217529297,
      "learning_rate": 0.00013076923076923077,
      "loss": 6.883045196533203,
      "step": 50
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 26.508798599243164,
      "learning_rate": 0.00011538461538461538,
      "loss": 7.3866020202636715,
      "step": 60
    },
    {
      "epoch": 0.5394990366088632,
      "grad_norm": 14.514946937561035,
      "learning_rate": 0.0001,
      "loss": 6.76031723022461,
      "step": 70
    },
    {
      "epoch": 0.6165703275529865,
      "grad_norm": 43.46797561645508,
      "learning_rate": 8.461538461538461e-05,
      "loss": 7.221854400634766,
      "step": 80
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 33.56849670410156,
      "learning_rate": 6.923076923076924e-05,
      "loss": 6.484788513183593,
      "step": 90
    },
    {
      "epoch": 0.7707129094412332,
      "grad_norm": 26.76589012145996,
      "learning_rate": 5.384615384615385e-05,
      "loss": 6.510005187988281,
      "step": 100
    },
    {
      "epoch": 0.8477842003853564,
      "grad_norm": 24.691797256469727,
      "learning_rate": 3.846153846153846e-05,
      "loss": 8.183999633789062,
      "step": 110
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 23.627254486083984,
      "learning_rate": 2.461538461538462e-05,
      "loss": 7.882224273681641,
      "step": 120
    },
    {
      "epoch": 1.0,
      "grad_norm": 143.46188354492188,
      "learning_rate": 9.230769230769232e-06,
      "loss": 6.064140701293946,
      "step": 130
    },
    {
      "epoch": 0.23648648648648649,
      "grad_norm": 0.0027358941733837128,
      "learning_rate": 0.00015472972972972976,
      "loss": 0.001809442602097988,
      "step": 140
    },
    {
      "epoch": 0.2533783783783784,
      "grad_norm": 0.00016119952488224953,
      "learning_rate": 0.00015135135135135137,
      "loss": 7.644749130122363e-05,
      "step": 150
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.00014990216004662216,
      "learning_rate": 0.00014797297297297298,
      "loss": 8.91086383489892e-06,
      "step": 160
    },
    {
      "epoch": 0.28716216216216217,
      "grad_norm": 0.00022610570886172354,
      "learning_rate": 0.00014459459459459462,
      "loss": 2.6643243472790346e-06,
      "step": 170
    },
    {
      "epoch": 0.30405405405405406,
      "grad_norm": 1.729255927784834e-05,
      "learning_rate": 0.0001412162162162162,
      "loss": 1.0013573955802713e-06,
      "step": 180
    },
    {
      "epoch": 0.32094594594594594,
      "grad_norm": 2.6247415007674135e-05,
      "learning_rate": 0.00013783783783783785,
      "loss": 1.8060190996038727e-06,
      "step": 190
    },
    {
      "epoch": 0.33783783783783783,
      "grad_norm": 0.0002764707896858454,
      "learning_rate": 0.00013445945945945946,
      "loss": 1.9669516404974276e-06,
      "step": 200
    },
    {
      "epoch": 0.3547297297297297,
      "grad_norm": 0.0002520938578527421,
      "learning_rate": 0.00013108108108108107,
      "loss": 2.2828560759080572e-06,
      "step": 210
    },
    {
      "epoch": 0.3716216216216216,
      "grad_norm": 7.781780732329935e-05,
      "learning_rate": 0.0001277027027027027,
      "loss": 1.555680137244053e-06,
      "step": 220
    },
    {
      "epoch": 0.3885135135135135,
      "grad_norm": 0.00023218011483550072,
      "learning_rate": 0.00012432432432432433,
      "loss": 1.0311597179679667e-06,
      "step": 230
    },
    {
      "epoch": 0.40540540540540543,
      "grad_norm": 0.00011573205119930208,
      "learning_rate": 0.00012094594594594595,
      "loss": 2.0682795366155914e-06,
      "step": 240
    },
    {
      "epoch": 0.4222972972972973,
      "grad_norm": 2.1888467017561197e-05,
      "learning_rate": 0.00011756756756756758,
      "loss": 8.58306339068804e-07,
      "step": 250
    },
    {
      "epoch": 0.4391891891891892,
      "grad_norm": 2.15825202758424e-05,
      "learning_rate": 0.00011418918918918919,
      "loss": 5.24520601175027e-07,
      "step": 260
    },
    {
      "epoch": 0.4560810810810811,
      "grad_norm": 0.00011002108658431098,
      "learning_rate": 0.00011081081081081082,
      "loss": 7.27176302461885e-07,
      "step": 270
    },
    {
      "epoch": 0.47297297297297297,
      "grad_norm": 2.175350527977571e-05,
      "learning_rate": 0.00010743243243243244,
      "loss": 7.927414117148146e-07,
      "step": 280
    },
    {
      "epoch": 0.48986486486486486,
      "grad_norm": 1.9356823031557724e-05,
      "learning_rate": 0.00010405405405405406,
      "loss": 6.735321221640334e-07,
      "step": 290
    },
    {
      "epoch": 0.5067567567567568,
      "grad_norm": 5.314841473591514e-05,
      "learning_rate": 0.00010067567567567568,
      "loss": 1.4662734429293777e-06,
      "step": 300
    },
    {
      "epoch": 0.5236486486486487,
      "grad_norm": 0.00010044712689705193,
      "learning_rate": 9.729729729729731e-05,
      "loss": 6.020066393830348e-07,
      "step": 310
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 9.594145376468077e-05,
      "learning_rate": 9.391891891891892e-05,
      "loss": 7.927413207653445e-07,
      "step": 320
    },
    {
      "epoch": 0.5574324324324325,
      "grad_norm": 1.847487328632269e-05,
      "learning_rate": 9.054054054054055e-05,
      "loss": 2.920626684499439e-07,
      "step": 330
    },
    {
      "epoch": 0.5743243243243243,
      "grad_norm": 0.00010169114102609456,
      "learning_rate": 8.716216216216216e-05,
      "loss": 1.078843433788279e-06,
      "step": 340
    },
    {
      "epoch": 0.5912162162162162,
      "grad_norm": 1.2515838534454815e-05,
      "learning_rate": 8.378378378378379e-05,
      "loss": 1.186131703434512e-06,
      "step": 350
    },
    {
      "epoch": 0.6081081081081081,
      "grad_norm": 0.00017822920926846564,
      "learning_rate": 8.040540540540541e-05,
      "loss": 1.1622898455243557e-06,
      "step": 360
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.000204792944714427,
      "learning_rate": 7.702702702702703e-05,
      "loss": 9.298320037487428e-07,
      "step": 370
    },
    {
      "epoch": 0.6418918918918919,
      "grad_norm": 8.428985893260688e-05,
      "learning_rate": 7.364864864864865e-05,
      "loss": 7.808204827597364e-07,
      "step": 380
    },
    {
      "epoch": 0.6587837837837838,
      "grad_norm": 6.9523295678664e-05,
      "learning_rate": 7.027027027027028e-05,
      "loss": 7.092949545040029e-07,
      "step": 390
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 2.688693166419398e-05,
      "learning_rate": 6.68918918918919e-05,
      "loss": 5.960461749054957e-07,
      "step": 400
    },
    {
      "epoch": 0.6925675675675675,
      "grad_norm": 1.2106567737646401e-05,
      "learning_rate": 6.351351351351352e-05,
      "loss": 9.000297723105177e-07,
      "step": 410
    },
    {
      "epoch": 0.7094594594594594,
      "grad_norm": 0.0001473653974244371,
      "learning_rate": 6.013513513513514e-05,
      "loss": 1.0550017577770631e-06,
      "step": 420
    },
    {
      "epoch": 0.7263513513513513,
      "grad_norm": 0.0001962837268365547,
      "learning_rate": 5.6756756756756757e-05,
      "loss": 4.7683697630418463e-07,
      "step": 430
    },
    {
      "epoch": 0.7432432432432432,
      "grad_norm": 0.00022100223577581346,
      "learning_rate": 5.337837837837838e-05,
      "loss": 6.854531420685816e-07,
      "step": 440
    },
    {
      "epoch": 0.7601351351351351,
      "grad_norm": 1.2869575584772974e-05,
      "learning_rate": 5e-05,
      "loss": 7.987018761923537e-07,
      "step": 450
    },
    {
      "epoch": 0.777027027027027,
      "grad_norm": 1.1445031304901931e-05,
      "learning_rate": 4.662162162162162e-05,
      "loss": 5.483625045599183e-07,
      "step": 460
    },
    {
      "epoch": 0.793918918918919,
      "grad_norm": 3.5025444958591834e-05,
      "learning_rate": 4.324324324324325e-05,
      "loss": 6.556509106303566e-07,
      "step": 470
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 1.604813769517932e-05,
      "learning_rate": 3.986486486486487e-05,
      "loss": 1.917306362884119e-05,
      "step": 480
    },
    {
      "epoch": 0.8277027027027027,
      "grad_norm": 1.2491855159169063e-05,
      "learning_rate": 3.648648648648649e-05,
      "loss": 6.854531420685816e-07,
      "step": 490
    },
    {
      "epoch": 0.8445945945945946,
      "grad_norm": 4.501197690842673e-05,
      "learning_rate": 3.310810810810811e-05,
      "loss": 7.152555099310121e-07,
      "step": 500
    },
    {
      "epoch": 0.8614864864864865,
      "grad_norm": 1.0608197953843046e-05,
      "learning_rate": 2.9729729729729733e-05,
      "loss": 5.781648269476136e-07,
      "step": 510
    },
    {
      "epoch": 0.8783783783783784,
      "grad_norm": 0.000130523883854039,
      "learning_rate": 2.635135135135135e-05,
      "loss": 8.34464663057588e-07,
      "step": 520
    },
    {
      "epoch": 0.8952702702702703,
      "grad_norm": 0.00013257596583571285,
      "learning_rate": 2.2972972972972976e-05,
      "loss": 1.7599831335246562e-05,
      "step": 530
    },
    {
      "epoch": 0.9121621621621622,
      "grad_norm": 7.942398224258795e-05,
      "learning_rate": 1.9594594594594595e-05,
      "loss": 9.775158105185255e-07,
      "step": 540
    },
    {
      "epoch": 0.9290540540540541,
      "grad_norm": 6.491731619462371e-05,
      "learning_rate": 1.6216216216216218e-05,
      "loss": 1.9840527966152876e-05,
      "step": 550
    },
    {
      "epoch": 0.9459459459459459,
      "grad_norm": 0.00013071534340269864,
      "learning_rate": 1.2837837837837838e-05,
      "loss": 6.020066848577698e-07,
      "step": 560
    },
    {
      "epoch": 0.9628378378378378,
      "grad_norm": 0.00784398429095745,
      "learning_rate": 9.45945945945946e-06,
      "loss": 3.5056215710937974e-05,
      "step": 570
    },
    {
      "epoch": 0.9797297297297297,
      "grad_norm": 7.595545321237296e-05,
      "learning_rate": 6.081081081081082e-06,
      "loss": 8.344647540070582e-07,
      "step": 580
    },
    {
      "epoch": 0.9966216216216216,
      "grad_norm": 4.217566311126575e-05,
      "learning_rate": 2.702702702702703e-06,
      "loss": 3.755091483981232e-07,
      "step": 590
    },
    {
      "epoch": 1.0,
      "eval_loss": 9.529818498776876e-07,
      "eval_runtime": 49.2882,
      "eval_samples_per_second": 1.927,
      "eval_steps_per_second": 1.927,
      "step": 592
    }
  ],
  "logging_steps": 10,
  "max_steps": 592,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.909877884761702e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
