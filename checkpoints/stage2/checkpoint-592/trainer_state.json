{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 592,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 10.71449089050293,
      "learning_rate": 0.00019846153846153847,
      "loss": 22.655870056152345,
      "step": 10
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 10.232928276062012,
      "learning_rate": 0.00019461538461538463,
      "loss": 5.933728790283203,
      "step": 20
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 15.738134384155273,
      "learning_rate": 0.0001907692307692308,
      "loss": 5.803728103637695,
      "step": 30
    },
    {
      "epoch": 0.30828516377649323,
      "grad_norm": 13.918347358703613,
      "learning_rate": 0.00018692307692307693,
      "loss": 5.635154342651367,
      "step": 40
    },
    {
      "epoch": 0.3853564547206166,
      "grad_norm": 15.645668983459473,
      "learning_rate": 0.0001830769230769231,
      "loss": 5.672851181030273,
      "step": 50
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 9.409565925598145,
      "learning_rate": 0.00017923076923076925,
      "loss": 5.497320175170898,
      "step": 60
    },
    {
      "epoch": 0.5394990366088632,
      "grad_norm": 8.557103157043457,
      "learning_rate": 0.0001753846153846154,
      "loss": 5.850469589233398,
      "step": 70
    },
    {
      "epoch": 0.6165703275529865,
      "grad_norm": 7.390137195587158,
      "learning_rate": 0.00017153846153846153,
      "loss": 5.624962615966797,
      "step": 80
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 6.91805362701416,
      "learning_rate": 0.00016769230769230772,
      "loss": 5.702191162109375,
      "step": 90
    },
    {
      "epoch": 0.7707129094412332,
      "grad_norm": 8.740006446838379,
      "learning_rate": 0.00016384615384615385,
      "loss": 5.662079238891602,
      "step": 100
    },
    {
      "epoch": 0.8477842003853564,
      "grad_norm": 9.592401504516602,
      "learning_rate": 0.00016,
      "loss": 5.896202087402344,
      "step": 110
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 12.15601634979248,
      "learning_rate": 0.00015615384615384615,
      "loss": 5.440667343139649,
      "step": 120
    },
    {
      "epoch": 1.0,
      "grad_norm": 10.161372184753418,
      "learning_rate": 0.0001523076923076923,
      "loss": 5.485287094116211,
      "step": 130
    },
    {
      "epoch": 1.0770712909441233,
      "grad_norm": 9.676664352416992,
      "learning_rate": 0.00014846153846153847,
      "loss": 5.823320007324218,
      "step": 140
    },
    {
      "epoch": 1.1541425818882467,
      "grad_norm": 10.996055603027344,
      "learning_rate": 0.0001446153846153846,
      "loss": 5.596116256713867,
      "step": 150
    },
    {
      "epoch": 1.2312138728323698,
      "grad_norm": 4.644158363342285,
      "learning_rate": 0.0001407692307692308,
      "loss": 5.58727912902832,
      "step": 160
    },
    {
      "epoch": 1.3082851637764932,
      "grad_norm": 7.662019729614258,
      "learning_rate": 0.00013692307692307693,
      "loss": 5.501605606079101,
      "step": 170
    },
    {
      "epoch": 1.3853564547206165,
      "grad_norm": 10.551322937011719,
      "learning_rate": 0.00013307692307692307,
      "loss": 5.762403106689453,
      "step": 180
    },
    {
      "epoch": 1.4624277456647399,
      "grad_norm": 5.634364128112793,
      "learning_rate": 0.00012923076923076923,
      "loss": 5.585348510742188,
      "step": 190
    },
    {
      "epoch": 1.5394990366088632,
      "grad_norm": 8.315348625183105,
      "learning_rate": 0.0001253846153846154,
      "loss": 5.573325729370117,
      "step": 200
    },
    {
      "epoch": 1.6165703275529864,
      "grad_norm": 9.58388900756836,
      "learning_rate": 0.00012153846153846153,
      "loss": 5.675560379028321,
      "step": 210
    },
    {
      "epoch": 1.69364161849711,
      "grad_norm": 6.323094844818115,
      "learning_rate": 0.0001176923076923077,
      "loss": 5.519981384277344,
      "step": 220
    },
    {
      "epoch": 1.770712909441233,
      "grad_norm": 10.932339668273926,
      "learning_rate": 0.00011384615384615384,
      "loss": 5.6203857421875,
      "step": 230
    },
    {
      "epoch": 1.8477842003853564,
      "grad_norm": 4.912787437438965,
      "learning_rate": 0.00011000000000000002,
      "loss": 5.724158477783203,
      "step": 240
    },
    {
      "epoch": 1.9248554913294798,
      "grad_norm": 7.204580307006836,
      "learning_rate": 0.00010615384615384615,
      "loss": 5.380632400512695,
      "step": 250
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.543238639831543,
      "learning_rate": 0.0001023076923076923,
      "loss": 5.320892715454102,
      "step": 260
    },
    {
      "epoch": 2.077071290944123,
      "grad_norm": 11.830482482910156,
      "learning_rate": 9.846153846153848e-05,
      "loss": 5.4537403106689455,
      "step": 270
    },
    {
      "epoch": 2.1541425818882467,
      "grad_norm": 10.95028305053711,
      "learning_rate": 9.461538461538461e-05,
      "loss": 5.461973190307617,
      "step": 280
    },
    {
      "epoch": 2.23121387283237,
      "grad_norm": 11.872149467468262,
      "learning_rate": 9.076923076923078e-05,
      "loss": 5.336160278320312,
      "step": 290
    },
    {
      "epoch": 2.3082851637764934,
      "grad_norm": 36.60530090332031,
      "learning_rate": 8.692307692307692e-05,
      "loss": 5.332376098632812,
      "step": 300
    },
    {
      "epoch": 2.3853564547206165,
      "grad_norm": 15.51420783996582,
      "learning_rate": 8.307692307692309e-05,
      "loss": 5.194593811035157,
      "step": 310
    },
    {
      "epoch": 2.4624277456647397,
      "grad_norm": 13.756877899169922,
      "learning_rate": 7.923076923076924e-05,
      "loss": 4.796422958374023,
      "step": 320
    },
    {
      "epoch": 2.5394990366088632,
      "grad_norm": 12.61147403717041,
      "learning_rate": 7.538461538461539e-05,
      "loss": 4.466828918457031,
      "step": 330
    },
    {
      "epoch": 2.6165703275529864,
      "grad_norm": 8.995928764343262,
      "learning_rate": 7.153846153846155e-05,
      "loss": 4.950369262695313,
      "step": 340
    },
    {
      "epoch": 2.69364161849711,
      "grad_norm": 13.254773139953613,
      "learning_rate": 6.76923076923077e-05,
      "loss": 4.582786178588867,
      "step": 350
    },
    {
      "epoch": 2.770712909441233,
      "grad_norm": 8.542901039123535,
      "learning_rate": 6.384615384615385e-05,
      "loss": 3.856131744384766,
      "step": 360
    },
    {
      "epoch": 2.847784200385356,
      "grad_norm": 14.116633415222168,
      "learning_rate": 6e-05,
      "loss": 5.095643997192383,
      "step": 370
    },
    {
      "epoch": 2.9248554913294798,
      "grad_norm": 8.257776260375977,
      "learning_rate": 5.615384615384616e-05,
      "loss": 4.092145538330078,
      "step": 380
    },
    {
      "epoch": 3.0,
      "grad_norm": 6.7490105628967285,
      "learning_rate": 5.230769230769231e-05,
      "loss": 4.387210464477539,
      "step": 390
    },
    {
      "epoch": 3.077071290944123,
      "grad_norm": 12.270012855529785,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.698662185668946,
      "step": 400
    },
    {
      "epoch": 3.1541425818882467,
      "grad_norm": 6.502784252166748,
      "learning_rate": 4.461538461538462e-05,
      "loss": 4.627236938476562,
      "step": 410
    },
    {
      "epoch": 3.23121387283237,
      "grad_norm": 8.222833633422852,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 4.010606384277343,
      "step": 420
    },
    {
      "epoch": 3.3082851637764934,
      "grad_norm": 7.567965984344482,
      "learning_rate": 3.692307692307693e-05,
      "loss": 4.1514404296875,
      "step": 430
    },
    {
      "epoch": 3.3853564547206165,
      "grad_norm": 7.152225017547607,
      "learning_rate": 3.307692307692308e-05,
      "loss": 4.1158802032470705,
      "step": 440
    },
    {
      "epoch": 3.4624277456647397,
      "grad_norm": 16.045753479003906,
      "learning_rate": 2.9230769230769234e-05,
      "loss": 4.673787307739258,
      "step": 450
    },
    {
      "epoch": 3.5394990366088632,
      "grad_norm": 17.32982635498047,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 4.605212783813476,
      "step": 460
    },
    {
      "epoch": 3.6165703275529864,
      "grad_norm": 11.200087547302246,
      "learning_rate": 2.1538461538461542e-05,
      "loss": 4.514555358886719,
      "step": 470
    },
    {
      "epoch": 3.69364161849711,
      "grad_norm": 5.815554141998291,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 3.818341827392578,
      "step": 480
    },
    {
      "epoch": 3.770712909441233,
      "grad_norm": 7.561807632446289,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 4.34716682434082,
      "step": 490
    },
    {
      "epoch": 3.847784200385356,
      "grad_norm": 6.804076194763184,
      "learning_rate": 1e-05,
      "loss": 4.3643135070800785,
      "step": 500
    },
    {
      "epoch": 3.9248554913294798,
      "grad_norm": 7.616644859313965,
      "learning_rate": 6.153846153846155e-06,
      "loss": 4.588748168945313,
      "step": 510
    },
    {
      "epoch": 4.0,
      "grad_norm": 8.421865463256836,
      "learning_rate": 2.307692307692308e-06,
      "loss": 4.715376281738282,
      "step": 520
    },
    {
      "epoch": 0.8952702702702703,
      "grad_norm": 6.506216526031494,
      "learning_rate": 2.2972972972972976e-05,
      "loss": 2.554765319824219,
      "step": 530
    },
    {
      "epoch": 0.9121621621621622,
      "grad_norm": 4.969086647033691,
      "learning_rate": 1.9594594594594595e-05,
      "loss": 2.9896955490112305,
      "step": 540
    },
    {
      "epoch": 0.9290540540540541,
      "grad_norm": 5.396946907043457,
      "learning_rate": 1.6216216216216218e-05,
      "loss": 2.9329818725585937,
      "step": 550
    },
    {
      "epoch": 0.9459459459459459,
      "grad_norm": 4.925870895385742,
      "learning_rate": 1.2837837837837838e-05,
      "loss": 3.0627408981323243,
      "step": 560
    },
    {
      "epoch": 0.9628378378378378,
      "grad_norm": 6.666253566741943,
      "learning_rate": 9.45945945945946e-06,
      "loss": 3.4009563446044924,
      "step": 570
    },
    {
      "epoch": 0.9797297297297297,
      "grad_norm": 3.9933927059173584,
      "learning_rate": 6.081081081081082e-06,
      "loss": 2.561388397216797,
      "step": 580
    },
    {
      "epoch": 0.9966216216216216,
      "grad_norm": 5.761336803436279,
      "learning_rate": 2.702702702702703e-06,
      "loss": 2.7994741439819335,
      "step": 590
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.5834808945655823,
      "eval_runtime": 31.1007,
      "eval_samples_per_second": 3.055,
      "eval_steps_per_second": 3.055,
      "step": 592
    }
  ],
  "logging_steps": 10,
  "max_steps": 592,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.355902001324851e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
